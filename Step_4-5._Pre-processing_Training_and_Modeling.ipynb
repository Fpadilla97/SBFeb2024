{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0efd8963",
   "metadata": {},
   "source": [
    "# Step 4:  Pre-processing and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55e06bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fedcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Cleaned_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d119bfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1330 entries, 0 to 1329\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Unnamed: 0      1330 non-null   int64         \n",
      " 1   Region          1330 non-null   object        \n",
      " 2   Country         1330 non-null   object        \n",
      " 3   Item_Type       1330 non-null   object        \n",
      " 4   Sales_Channel   1330 non-null   object        \n",
      " 5   Order_Priority  1330 non-null   object        \n",
      " 6   Order_Date      1330 non-null   datetime64[ns]\n",
      " 7   Order_ID        1330 non-null   int64         \n",
      " 8   Ship_Date       1330 non-null   datetime64[ns]\n",
      " 9   Units_Sold      1330 non-null   int64         \n",
      " 10  Unit_Price      1330 non-null   float64       \n",
      " 11  Unit_Cost       1330 non-null   float64       \n",
      " 12  Total_Revenue   1330 non-null   float64       \n",
      " 13  Total_Cost      1330 non-null   float64       \n",
      " 14  Total_Profit    1330 non-null   float64       \n",
      " 15  Shipping_Days   1330 non-null   int64         \n",
      "dtypes: datetime64[ns](2), float64(5), int64(4), object(5)\n",
      "memory usage: 166.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21761c26",
   "metadata": {},
   "source": [
    "# Dropping the Unnecessary Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb7fd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Region', 'Country', 'Item_Type', 'Sales_Channel', 'Order_Priority',\n",
       "       'Order_Date', 'Order_ID', 'Ship_Date', 'Units_Sold', 'Unit_Price',\n",
       "       'Unit_Cost', 'Total_Revenue', 'Total_Cost', 'Total_Profit',\n",
       "       'Shipping_Days'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the 'Unnamed' column as it is just an index column\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4137b95",
   "metadata": {},
   "source": [
    "# Creating Dummy Variables for Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfbfeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Order_Date', 'Order_ID', 'Ship_Date', 'Units_Sold', 'Unit_Price',\n",
       "       'Unit_Cost', 'Total_Revenue', 'Total_Cost', 'Total_Profit',\n",
       "       'Shipping_Days', 'Country_Andorra', 'Country_Armenia',\n",
       "       'Country_Austria', 'Country_Belarus', 'Country_Belgium',\n",
       "       'Country_Bosnia and Herzegovina', 'Country_Bulgaria', 'Country_Croatia',\n",
       "       'Country_Cyprus', 'Country_Czech Republic', 'Country_Denmark',\n",
       "       'Country_Estonia', 'Country_Finland', 'Country_France',\n",
       "       'Country_Georgia', 'Country_Germany', 'Country_Greece',\n",
       "       'Country_Hungary', 'Country_Iceland', 'Country_Ireland',\n",
       "       'Country_Italy', 'Country_Kosovo', 'Country_Latvia',\n",
       "       'Country_Liechtenstein', 'Country_Lithuania', 'Country_Luxembourg',\n",
       "       'Country_Macedonia', 'Country_Malta', 'Country_Moldova ',\n",
       "       'Country_Monaco', 'Country_Montenegro', 'Country_Netherlands',\n",
       "       'Country_Norway', 'Country_Poland', 'Country_Portugal',\n",
       "       'Country_Romania', 'Country_Russia', 'Country_San Marino',\n",
       "       'Country_Serbia', 'Country_Slovakia', 'Country_Slovenia',\n",
       "       'Country_Spain', 'Country_Sweden', 'Country_Switzerland',\n",
       "       'Country_Ukraine', 'Country_United Kingdom', 'Country_Vatican City',\n",
       "       'Item_Type_Beverages', 'Item_Type_Cereal', 'Item_Type_Clothes',\n",
       "       'Item_Type_Cosmetics', 'Item_Type_Fruits', 'Item_Type_Household',\n",
       "       'Item_Type_Meat', 'Item_Type_Office Supplies',\n",
       "       'Item_Type_Personal Care', 'Item_Type_Snacks', 'Item_Type_Vegetables',\n",
       "       'Sales_Channel_Online', 'Order_Priority_H', 'Order_Priority_L',\n",
       "       'Order_Priority_M'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of categorical columns to encode\n",
    "categorical_cols = ['Region', 'Country', 'Item_Type', 'Sales_Channel', 'Order_Priority']\n",
    "\n",
    "# using One-hot encode for creating numerical dummy columns for categorical columns \n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "df_encoded.columns          # new column names after encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100827a",
   "metadata": {},
   "source": [
    "### Code over view:\n",
    "\n",
    "I have used **pd.get_dummies()** to convert categorical columns into numerical dummy variables.\n",
    "\n",
    "**drop_first=True** is used to avoid multicollinearity (it drops the first category as a baseline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebed56",
   "metadata": {},
   "source": [
    "# Standardizing the Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b3ccb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized numeric features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Units_Sold</th>\n",
       "      <th>Unit_Price</th>\n",
       "      <th>Unit_Cost</th>\n",
       "      <th>Total_Revenue</th>\n",
       "      <th>Total_Cost</th>\n",
       "      <th>Total_Profit</th>\n",
       "      <th>Shipping_Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.330000e+03</td>\n",
       "      <td>1.330000e+03</td>\n",
       "      <td>1.330000e+03</td>\n",
       "      <td>1.330000e+03</td>\n",
       "      <td>1.330000e+03</td>\n",
       "      <td>1.330000e+03</td>\n",
       "      <td>1.330000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.609548e-17</td>\n",
       "      <td>1.335607e-18</td>\n",
       "      <td>-7.746519e-17</td>\n",
       "      <td>-1.028417e-16</td>\n",
       "      <td>-2.737994e-17</td>\n",
       "      <td>2.938335e-17</td>\n",
       "      <td>6.410912e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000376e+00</td>\n",
       "      <td>1.000376e+00</td>\n",
       "      <td>1.000376e+00</td>\n",
       "      <td>1.000376e+00</td>\n",
       "      <td>1.000376e+00</td>\n",
       "      <td>1.000376e+00</td>\n",
       "      <td>1.000376e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.704131e+00</td>\n",
       "      <td>-1.176402e+00</td>\n",
       "      <td>-1.024045e+00</td>\n",
       "      <td>-9.791042e-01</td>\n",
       "      <td>-9.385844e-01</td>\n",
       "      <td>-1.075377e+00</td>\n",
       "      <td>-1.700192e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.916129e-01</td>\n",
       "      <td>-8.431323e-01</td>\n",
       "      <td>-8.598134e-01</td>\n",
       "      <td>-7.620871e-01</td>\n",
       "      <td>-7.506134e-01</td>\n",
       "      <td>-8.277662e-01</td>\n",
       "      <td>-8.767197e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.060859e-03</td>\n",
       "      <td>-5.101853e-01</td>\n",
       "      <td>-5.099975e-01</td>\n",
       "      <td>-3.574975e-01</td>\n",
       "      <td>-4.217324e-01</td>\n",
       "      <td>-2.758673e-01</td>\n",
       "      <td>1.537562e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.644933e-01</td>\n",
       "      <td>7.931554e-01</td>\n",
       "      <td>4.320634e-01</td>\n",
       "      <td>4.443894e-01</td>\n",
       "      <td>3.844298e-01</td>\n",
       "      <td>5.253938e-01</td>\n",
       "      <td>8.388482e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.739532e+00</td>\n",
       "      <td>1.856809e+00</td>\n",
       "      <td>1.917816e+00</td>\n",
       "      <td>2.254104e+00</td>\n",
       "      <td>2.086995e+00</td>\n",
       "      <td>2.555134e+00</td>\n",
       "      <td>1.730944e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Units_Sold    Unit_Price     Unit_Cost  Total_Revenue    Total_Cost  \\\n",
       "count  1.330000e+03  1.330000e+03  1.330000e+03   1.330000e+03  1.330000e+03   \n",
       "mean  -5.609548e-17  1.335607e-18 -7.746519e-17  -1.028417e-16 -2.737994e-17   \n",
       "std    1.000376e+00  1.000376e+00  1.000376e+00   1.000376e+00  1.000376e+00   \n",
       "min   -1.704131e+00 -1.176402e+00 -1.024045e+00  -9.791042e-01 -9.385844e-01   \n",
       "25%   -8.916129e-01 -8.431323e-01 -8.598134e-01  -7.620871e-01 -7.506134e-01   \n",
       "50%    3.060859e-03 -5.101853e-01 -5.099975e-01  -3.574975e-01 -4.217324e-01   \n",
       "75%    8.644933e-01  7.931554e-01  4.320634e-01   4.443894e-01  3.844298e-01   \n",
       "max    1.739532e+00  1.856809e+00  1.917816e+00   2.254104e+00  2.086995e+00   \n",
       "\n",
       "       Total_Profit  Shipping_Days  \n",
       "count  1.330000e+03   1.330000e+03  \n",
       "mean   2.938335e-17   6.410912e-17  \n",
       "std    1.000376e+00   1.000376e+00  \n",
       "min   -1.075377e+00  -1.700192e+00  \n",
       "25%   -8.277662e-01  -8.767197e-01  \n",
       "50%   -2.758673e-01   1.537562e-02  \n",
       "75%    5.253938e-01   8.388482e-01  \n",
       "max    2.555134e+00   1.730944e+00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler            # library for standard Scaling\n",
    "\n",
    "# List of numerical columns for standardization\n",
    "numeric_cols = ['Units_Sold', 'Unit_Price', 'Unit_Cost', 'Total_Revenue', 'Total_Cost', 'Total_Profit', 'Shipping_Days']\n",
    "\n",
    "# Initializing the StandardScaler as scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Applying standardization to the numerical columns\n",
    "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "\n",
    "# Displaying a summary of the standardized data\n",
    "print(\"Standardized numeric features.\")\n",
    "\n",
    "df_encoded[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5daa23b",
   "metadata": {},
   "source": [
    "### Code over view:\n",
    "\n",
    "I have used StandardScaler to ensure all numerical columns have a mean of 0 and standard deviation of 1, and as you can observe that this goal is achieved.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Over all this step is crucial for many machine learning algorithms that are sensitive to the scale of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0453e35",
   "metadata": {},
   "source": [
    "# Spliting Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fcb7e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (1064, 68)\n",
      "Testing features shape: (266, 68)\n",
      "Training target shape: (1064,)\n",
      "Testing target shape: (266,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split           # library to generate/split data into test and train\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Drop unnecessary columns\n",
    "df = df.drop(columns=['Order_ID','Ship_Date', 'Total_Revenue', 'Total_Cost'])\n",
    "\n",
    "\n",
    "# Step 2: Extract features from 'Order_Date'\n",
    "df['Year'] = df['Order_Date'].dt.year\n",
    "df['Month'] = df['Order_Date'].dt.month\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: One-hot encode categorical columns\n",
    "categorical_cols = ['Region', 'Country', 'Item_Type', 'Sales_Channel', 'Order_Priority']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "# Step 4: Standardize numerical features\n",
    "numeric_cols = ['Units_Sold', 'Unit_Price', 'Unit_Cost', 'Shipping_Days', 'Year', 'Month']\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "\n",
    "\n",
    "# Step 5: Define features (X) and target (y)\n",
    "X = df_encoded.drop(columns=['Total_Profit', 'Order_Date'])     # Use all features except 'Total_Profit'\n",
    "y = df_encoded['Total_Profit']\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=14)\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Testing target shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c0e7a",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "### step 1:\n",
    "\n",
    "Removing Total_Revenue and Total_Cost to avoid data leakage, as they can be derived from other columns.\n",
    "\n",
    "Removing Ship_Date since they are redundant when Order_Date and Shipping_Days is already included.\n",
    "\n",
    "Removing Order_ID column as this column is just representing unique index for each order.\n",
    "\n",
    "**By excluding these redundant columns, I have reduce the risk of overfitting and ensure that the model learns meaningful patterns rather than relying on direct relationships.**\n",
    "\n",
    "### step 2:\n",
    "\n",
    "Extracting year and month from Order_Date column so that model can capture seasonal patterns and trends that can be crucial for sales predictions.\n",
    "\n",
    "### step 3:\n",
    "\n",
    "creating dummy variabels by using One-hot encode categorical columns as (Region, Country, Item_Type, Sales_Channel, Order_Priority).\n",
    "\n",
    "### step 4:\n",
    "\n",
    "standardizing the numerical fetures which means to change all numerical column's unit one sigle/same unit or in simple words you can say that making one single unit for all numerical columns.\n",
    "\n",
    "### step 5:\n",
    "\n",
    "defining the features and target columns and saving in X and y data respectively.\n",
    "\n",
    "### step 6:\n",
    "\n",
    "Now finally splitting the data into training and test data set where training data is of 80% and testing data is 20% and random_state define that the sample taken for training and test data has been fixed at random_state 14.\n",
    "\n",
    "**NOTE:** You can change random_state as per your choise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358d602",
   "metadata": {},
   "source": [
    "# Step 5: Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ef962",
   "metadata": {},
   "source": [
    "### importing essential libraries for different model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e999f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb1255",
   "metadata": {},
   "source": [
    "## Developing a function which calculates the matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a1aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating R² and RMSE\n",
    "def calculate_metrics(y_true, y_pred):                  ## inputs: True target value as y_true and predicted value as y_pred\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return r2, rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d1a9e",
   "metadata": {},
   "source": [
    "## Model 1: Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d06b191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: Linear Regression\n",
      "R² (Train): 0.85, R² (Test): 0.87\n",
      "RMSE (Train): 135484.49, RMSE (Test): 120776.40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegression()               # creating modelobject\n",
    "\n",
    "# Fitting and predicting\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_train_linear = linear_model.predict(X_train)\n",
    "y_pred_test_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluating model metrics using calculate_metrics function define in above step\n",
    "r2_train_linear, rmse_train_linear = calculate_metrics(y_train, y_pred_train_linear)\n",
    "r2_test_linear, rmse_test_linear = calculate_metrics(y_test, y_pred_test_linear)\n",
    "\n",
    "print(\"Model 1: Linear Regression\")\n",
    "print(f\"R² (Train): {r2_train_linear:.2f}, R² (Test): {r2_test_linear:.2f}\")\n",
    "print(f\"RMSE (Train): {rmse_train_linear:.2f}, RMSE (Test): {rmse_test_linear:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df25aa",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest Regressor with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c0eeb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 10}\n",
      "Model 2: Random Forest Regressor\n",
      "R² (Train): 0.91, R² (Test): 0.90\n",
      "RMSE (Train): 104655.93, RMSE (Test): 106013.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(random_state=14)  ## creating model object at random state 14\n",
    "\n",
    "# Defining hyperparameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [5, 10],  # setting number of trees\n",
    "    'max_depth': [3],        # setting depth\n",
    "    'min_samples_split': [10, 15]  #  setting samples split\n",
    "}\n",
    "\n",
    "# Performing Grid Search for the best hyperparameters\n",
    "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=3, scoring='r2', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best estimator from Grid Search\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "print(f\"Best Random Forest Parameters: {grid_search_rf.best_params_}\")\n",
    "\n",
    "# Fit and predict with the best model\n",
    "y_pred_train_rf = best_rf_model.predict(X_train)\n",
    "y_pred_test_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluating model metrics usinf function\n",
    "r2_train_rf, rmse_train_rf = calculate_metrics(y_train, y_pred_train_rf)\n",
    "r2_test_rf, rmse_test_rf = calculate_metrics(y_test, y_pred_test_rf)\n",
    "\n",
    "## final metric results\n",
    "print(\"Model 2: Random Forest Regressor\")\n",
    "print(f\"R² (Train): {r2_train_rf:.2f}, R² (Test): {r2_test_rf:.2f}\")\n",
    "print(f\"RMSE (Train): {rmse_train_rf:.2f}, RMSE (Test): {rmse_test_rf:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ff80a",
   "metadata": {},
   "source": [
    "## Model 3: Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "201c8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3: Decision Tree Regressor\n",
      "R² (Train): 0.89, R² (Test): 0.88\n",
      "RMSE (Train): 113483.33, RMSE (Test): 115176.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeRegressor(max_depth=3, random_state=15) \n",
    "                                                        # Limiting the depth to avoid overfitting, same depth as of model 2\n",
    "\n",
    "# Fiting and predicting\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_train_dt = dt_model.predict(X_train)\n",
    "y_pred_test_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluating model\n",
    "r2_train_dt, rmse_train_dt = calculate_metrics(y_train, y_pred_train_dt)\n",
    "r2_test_dt, rmse_test_dt = calculate_metrics(y_test, y_pred_test_dt)\n",
    "\n",
    "\n",
    "## observing metrics\n",
    "print(\"Model 3: Decision Tree Regressor\")\n",
    "print(f\"R² (Train): {r2_train_dt:.2f}, R² (Test): {r2_test_dt:.2f}\")\n",
    "print(f\"RMSE (Train): {rmse_train_dt:.2f}, RMSE (Test): {rmse_test_dt:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b82be",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b425600",
   "metadata": {},
   "source": [
    "The goal of this project was to build predictive models to estimate **Total Profit** based on various features from a sales dataset. As I explored three regression models: **Linear Regression**, **Random Forest Regressor**, and **Decision Tree Regressor**. Each model was evaluated based on its performance on both the training and testing datasets using **R²** (coefficient of determination) and **RMSE** (Root Mean Squared Error) as evaluation metrics.\n",
    "\n",
    "### **Model Performance Summary**\n",
    "\n",
    "| Model                     | R² (Train) | R² (Test) | RMSE (Train) | RMSE (Test) |\n",
    "|---------------------------|------------|-----------|--------------|-------------|\n",
    "| Linear Regression         | 0.85       | 0.87      | 135,484.49   | 120,776.40  |\n",
    "| Random Forest Regressor   | 0.91       | 0.90      | 104,655.93   | 106,013.93  |\n",
    "| Decision Tree Regressor   | 0.89       | 0.88      | 113,483.33   | 115,176.42  |\n",
    "\n",
    "**Important Note: As I have use same depth of 3 to make fine comparision between Random Forest & Decision Tree model but you can set the depth as your own to observe more precision in both of the models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019a376",
   "metadata": {},
   "source": [
    "### Analysis of Each Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faf7f1",
   "metadata": {},
   "source": [
    "1. **Linear Regression**:\n",
    "   - This model served as the baseline. With an R² score of **0.87** on the test data, it performed well in capturing the linear relationships between the features and the target variable.\n",
    "   - The **RMSE** of **120,776.40** on the test data indicates the average error in predicting Total Profit.\n",
    "   - The model's lower complexity makes it easy to interpret, but it may not capture non-linear relationships effectively.\n",
    "\n",
    "2. **Random Forest Regressor**:\n",
    "   - The Random Forest model provided the best performance, with an R² score of **0.90** on the test data.\n",
    "   - The **RMSE** of **106,013.93** was the lowest among the three models, indicating more accurate predictions.\n",
    "   - This model benefited from **hyperparameter tuning** (`max_depth=3`, `min_samples_split=10`, `n_estimators=10`), which helped prevent overfitting while still capturing complex patterns in the data.\n",
    "\n",
    "3. **Decision Tree Regressor**:\n",
    "   - The Decision Tree model achieved an R² score of **0.88** on the test data, slightly lower than Random Forest but still competitive.\n",
    "   - The **RMSE** was **115,176.42**, which is higher than that of the Random Forest but better than the baseline Linear Regression model.\n",
    "   - Limiting the tree depth (`max_depth=3`) (same as in Random Forest Regressor) helped reduce overfitting, but this model can still be prone to capturing noise in the data compared to ensemble methods like Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39310e9c",
   "metadata": {},
   "source": [
    "### Best Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4c6ee",
   "metadata": {},
   "source": [
    "\n",
    "- **Best Model**: The **Random Forest Regressor** is the best-performing model based on both R² and RMSE scores. It balances bias and variance well, providing accurate predictions without overfitting.\n",
    "- **Second Choice**: The **Decision Tree Regressor** is a good alternative if interpretability is important, as it offers a more transparent decision-making process compared to Random Forest.\n",
    "- **Baseline Model**: The **Linear Regression** model serves as a useful baseline, demonstrating strong performance while being simple and interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97b951",
   "metadata": {},
   "source": [
    "### Analysis of Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4b745",
   "metadata": {},
   "source": [
    "**Does my data involve a time series or forecasting? If so, am I splitting the train and test data appropriately?**\n",
    "\n",
    "\n",
    "  - Yes, the dataset includes a time-based feature (`Order_Date`), from which we extracted **Year** and **Month** for potential seasonal patterns.\n",
    "  - However, this project did not explicitly use a time series or forecasting approach (e.g., time-based splitting or rolling forecasting origin). Instead, I used a random split (`train_test_split`) to divide the data into training and testing sets.\n",
    "  - This method is appropriate in this context because:\n",
    "    - As I included time-based features in the model (Year and Month), allowing it to capture some time-related patterns.\n",
    "    - Random splitting was chosen due to the lack of explicit sequential dependence (e.g., future predictions based purely on historical data).\n",
    "\n",
    "\n",
    "**Is my response variable continuous or categorical?**\n",
    "\n",
    "\n",
    "  - The response variable, **Total Profit**, is a **continuous numeric variable**. It may represents a dollar amount that can take on any value within a certain range, making this a **regression problem**.\n",
    "  - Given the continuous nature of the target variable, I have chosed regression models (Linear Regression, Random Forest Regressor, Decision Tree Regressor) to predict it effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548f86f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7833998",
   "metadata": {},
   "source": [
    "1. The **Random Forest Regressor** is the recommended model, as it provided the best balance between high accuracy (R² of 0.90) and low error (RMSE of 106,013.93).\n",
    "2. The features used in the model (including time-based features extracted from `Order_Date`) helped capture trends and patterns, contributing to improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253142b0",
   "metadata": {},
   "source": [
    "# xxxxxxxxxxxxxxxxxxxxxxxx(Thanks)xxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707fc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
